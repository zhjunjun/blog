分布式系统真的很棒。 但是，并非所有关于分布式系统的事情都是好的。 作为工程师，在实现和使用分布式系统来解决问题时需要格外小心。



事实是，实现一个分布式系统是一项非常艰巨的任务，它带来了灵活性和可伸缩性，也带来了复杂性，在某些情况下甚至带来了进一步的隐患。



人们使用分布式系统的常见原因：

- 高可用
- 可扩展
- 分区容错
- 独立部属
- 将不同技术用于不同目的



需要注意的是，分布式系统可能存在如下问题：

- 耗时
- 复杂性
- 一致性问题
- 延时



在本文中，将重点讨论延迟，尤其是尾延迟。 将讨论一些技术，并且还会展示两种技术的实际实现。 首先，需要了解一些概念。



## 延时

当数据通过网络分发系统时，延迟就会增加。 在网络上访问任何分布式系统的每个节点都不是免费的。 除了网络成本外，还涉及其他成本。 例如，如果系统使用的是通过HTTP进行通信，将有额外的开销来处理消息，解析消息，验证身份验证token以及我们想要添加到管道中的所有其他内容。 总之，设计分布式系统时要牢记一件事， 某一系统的额外的延迟是否真的值得？



为了回答这个问题，需要了解如何测量延迟。 最简单的答案之一就是使用百分位。



## 百分位

wiki中百分位的定义是这样的

> 百分位（或百分比）是统计中使用的一种度量，它指一组观察值中给定观察值的百分比低于该值。

换个人话：

首先要定义观察组。要衡量的是什么。对于延迟，最常见的观察组是给定请求类别的响应时间。

1、为一个给定的请求测试所有的响应时间。

2、取所有元素的前x%

3、获取所选集合的最大(最长)值。



在这个示例中，假设要针对网址`/ hello-world`的请求测量响应时间。 对该URL进行10次调用，并记下每个请求响应的时间。 因此，将得到如下（时间（以毫秒[ms]为单位））

```
23, 20, 21, 20, 23, 20, 45, 21, 25, 25
```

将上诉结果排序(时间以毫秒[ms]为单位)

```
20, 20, 20, 21, 21, 23, 23, 25, 25, 45
```

下一步是获取元素的前x％。 假设要获得第50个百分位。 因此，我们获得了前5个元素：

```
20, 20, 20, 21, 21
```

然后得到最大的值(最后一个)。我们的第50百分位数是21毫秒。p90(另一种写法)应该是25毫秒，因为25是我们示例中的第9个值。

这只是一个例子，对于实际的应用程序，可以使用测试工具进行测量。 有些工具具有开箱即用的api，因此只需使用这些工具来对应用程序进行测试，然后就可以使用了。



## 尾延迟

尾延迟指百分位数频谱末尾的延迟。 通常指超过99％的延时。 某些系统对99％的请求都有很好的响应。 从外观上看似乎还不错，因为它只有1％或更少的延迟。 但是，如果谈论的是每分钟接收数百万个请求的系统，那么这个数字并不是那么微不足道。 况且，你也可能不想成为那1％的用户。 如果仔细思考一下，其实很难有很好的尾部延迟百分位数，因为如果很少的请求受到某些因素的影响，则只会影响尾部。



这就是为什么人们研究并发表有关该论文的原因



# The tail at scale

该论文由Google员工于2013年发表。确切的说，Google已经处理了很长时间。 并非所有公司都有Google这样的流量负载，也不是所有系统都有延迟尾部问题。 需要特别注意的是，这些技术旨在解决一个非常具体的问题，因此，在没有认真对比它们的利弊之间并且进行认真而透彻的分析，就不应使用这些技术。 请记住，这些技术需要付出一定的成本，有时甚至是巨大的成本，并且需要权衡实现这些技术的实际好处。 好的，话虽如此，还是让我们直接深入其中吧。



这是一篇非常有趣的引文，它着重强调了这些问题的严重性。



> 完成所有请求的99％分位延迟为140ms，95％的请求完成的99％延迟为70ms，这意味着等待最慢5％的请求完成的请求占总99％的一半百分位数延迟



当根据响应时间的第99个百分位创建服务水平协议（SLA）时，通常会出现这种担忧。 也许你已经看到了SLA，甚至更常见的情况是SLA是基于百分比的正常运行时间。 在这些情况下，如果有将p99向下拖动（或在这种情况下向上拖动）的方法，确实会有作用，这就是你需要研究的。 说到SLA，就像小狗摇尾巴！



在这篇论文中，发布了7种处理尾部延迟的非常好的技术。 分别是：



- 对冲的请求
- 捆绑请求
- 微分区
- 选择性复制
- 潜伏期缓刑
- 足够好的响应
- 金丝雀要求



本文将专注于并实现第一种技术（以及另一种隐式技术）。 这是一种与代码直接相关的技术，因此很容易实现。 唯一需要的是正在使用的服务的响应时间信息。 而且我们已经知道如何获得这些信息。



## 对冲请求

处理分布式系统时，出于计算能力和分区容错（如果一个副本出现故障，则系统保持运行）的原因，通常会创建同一系统的副本。 关于此设置，从白皮书中获得以下建议：



> 抑制延迟可变性的一种简单方法是向多个副本发出相同的请求，并使用最先响应的副本的结果。



这很容易实现。 如果我们的系统是一个带有3个副本的特定系统，那我们会发出3个请求，每个副本一个，然后使用首先返回的结果。 这很有意义，因为所有3个副本同时表现不佳的可能性非常低（但并非不可能）。 这种方法的主要问题是成本。 随着系统扩展和添加更多副本，成本将增加。 发出n个请求很浪费，因为你知道你只会使用一个响应。 就需要你为每个请求消耗网络带宽以及计算资源。 成本很高。



对于性能至关重要的关键性高可用性系统，采用此方法。 像实时应用程序一样。 对于更广泛的用途，本文的作者提出了另一种方法，即对冲的请求，可以将其定义如下：



>  一种这样的方法是推迟发送辅助请求，直到第一个请求的未完成时间超过此类请求的预期延迟的95％。 这种方法将附加负载限制在大约5％，同时大大缩短了延迟时间。



这就是为什么你需要有你的应用程序的响应耗时，你应用程序的耗时的百分位是决定何时发出新请求的主要驱动力。 记住，你正在处理的应用程序的尾延时差异很大，因此第95个百分位数仍是一个可以接受的值



这是从论文中摘录的示例，展示了此技术的强大功能：



> 例如，在一个Google基准测试中，该基准测试了分布在100个不同服务器上的BigTable表中存储的1,000个键值对，在10ms延迟后发送对冲请求将将所有1,000个值从1,800ms检索到74ms的等待时间降低了99.9％ 同时仅发送2％的请求。



由于仅发出了2％以上的请求，因此具有更高的成本效益。 在该示例中，尾部等待时间的下降是荒谬的，并非所有系统都将以类似的方式运行，但是使用此技术仍可以得到一些不错的结果。



既然已经知道了使用此技术对尾部延迟的影响，现在我们来看看实现该技术有多么困难。



## 对冲请求在Golang中的实现

这是对冲请求和另一种技术的go实现（向所有副本发出请求并使用第一个结果返回）。 我在go中实现了该功能，因为我将go的并发模型与goroutine和通道结合使用。



在此示例中，我有一个存在3个副本的本地服务。 它正在我本地主机上的不同端口（8081、8082、8083）中运行。 该服务模拟尾部延迟问题。 这是硬编码的，因此在96％的时间内它可以正常工作，在4％的请求中它将有很长的`time.Sleep（）`。 

下面是该服务的主要实现

```go
package main

import (
	"math/rand"
	"net/http"
	"time"

	"github.com/gorilla/mux"
)

func main() {
	router := mux.NewRouter()

	router.HandleFunc("/ishealthy", func(w http.ResponseWriter, r *http.Request) {
		rd := rand.New(rand.NewSource(time.Now().UnixNano()))
		requestPercentile := rd.Intn(100)
		waitTime := 0

		if requestPercentile > 96 {
			waitTime = 100
		}

		time.Sleep(time.Duration(waitTime+15) * time.Millisecond)
		w.WriteHeader(http.StatusOK)
		w.Write([]byte("Healthy"))
	}).Methods(http.MethodGet)
	http.ListenAndServe(":8080", router)
}
```



然后用`[vegeta](https://github.com/tsenart/vegeta)`测试了一些指标

![Image for post](https://miro.medium.com/max/2936/1*ZogX2WbBdBka_pcqmxc5gQ.png)



如我们所见，我们的设置如下

- p50小于20毫秒
- p95大约20毫秒
- p99超过115毫秒



我将使用另一个Web应用程序作为该服务的客户端。 在该客户端内部，有3个不同的URL：

- `/ simple`-简单调用第一个副本URL。 应该具有与每个副本相同的性能。
- `/ fanout`-对于收到的每个请求，扇出3个请求（每个副本1个）。 应该在20ms左右具有最佳性能。 发出3倍以上的请求。
- `/ hedged`-在第一个请求未达到预期的P95（21毫秒）后触发对冲请求。 应该将尾性能提高到40毫秒左右的P99。 仅触发多达5％的请求。



该客户端仅向外部服务发出HTTP请求。 可以看到，对于收到的每个响应，我都会添加一个日志条目。 因此，很容易跟踪我们之后发出的请求数量。

```go
package main

import (
	"fmt"
	"io/ioutil"
	"net/http"
	"time"
  neturl "net/url"
)

func executeQuery(ctx context.Context, url string) string {
	start := time.Now()
  parsedURL, _ := neturl.Parse(url)
  req := &http.Request{URL: parsedURL}
  req = req.WithContext(ctx)
   	
  response, err := http.DefaultClient.Do(req)
  
  if err != nil {
		fmt.Println(err.Error())
		return err.Error()
	}
  
	defer response.Body.Close()

	body, _ := ioutil.ReadAll(response.Body)
	fmt.Printf("Request time: %d ms from url%s\n", time.Since(start).Nanoseconds()/time.Millisecond.Nanoseconds(), url)
	return fmt.Sprintf("%s from %s", body, url)
}
```



`/ fanout `url实现了本文介绍的第一种技术。

```go
package main

func queryFanOut(urls []string) string {
	ch := make(chan string, len(urls))
  ctx := context.Background()
	for _, url := range urls {
		go func(u string) {
			ch <- executeQuery(ctx, u)
		}(url)
	}
	return <-ch
}
```



为了进行测试，将`stdout`定向到文件中。 我还是使用`vegeta`针对`/ fanout`网址发出500个请求。 可以看到，对于收到的每个请求，它向外部服务发出3个请求（每个副本一个请求）。 因为我只使用第一个副本，所以可以看到尾延时下降了很多（大多数情况下，其中一个副本会在可接受的时间内响应）。

![Image for post](https://miro.medium.com/max/2876/1*BJWDQnSMHx1oNBN0s1J3Kg.png)



`/ hedged`实际上是对冲请求的实现。 这与`/fanout`的实现非常相似。 主要区别在于在发出后续请求之前等待21 ms。 并且不会取消第一个请求，因为它可能是误报，例如，它可能会在22毫秒后返回，因此将使用最先返回的响应。 同样的方法。

```go
package main

import (
	"time"
)

func queryWithHedgedRequests(urls []string) string {
	ch := make(chan string, len(urls))
  ctx, cancel := context.WithCancel(context.Background())
  defer cancel()
	for _, url := range urls {
		go func(u string, c chan string) {
			c <- executeQuery(u, ctx)
		}(url, ch)

		select {
		case r := <-ch:
      cancel()
			return r
		case <-time.After(21 * time.Millisecond):
		}
	}

	return <-ch
}
```

将运行与此之前完全相同的测试。 在这里可以看到p99从大约120毫秒下降到大约41毫秒， 同时仅发出3.6％的请求（18个请求）。



![Image for post](https://miro.medium.com/max/2856/1*BZP1tnw3VEbKZTXc-n9Qrg.png)



这是很合理的，因为在发出第二个请求之前等待21 ms。 因此，这是通常的20毫秒+ 21毫秒的等待时间。 比120毫秒好得多，发出的请求比`/ fanout`少得多。



如果在笔记本电脑上运行示例，这些值可能会有所不同。 因此，可能调整这些值以反映所看到的内容。



## 总结

仅用几行代码，就成功地大大提高了尾部延迟。在将该示例用作生产系统之前，还有很多地方需要改进，但核心实现与此没有太大区别。



